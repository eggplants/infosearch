# 03: 検索のためのテキスト処理

## 検索のためのテキスト処理

- 検索文書⇒索引語集合に変換したい
- 活用形，複数形，大文字・小文字のせいで、クエリから検索漏れする場合
  - 形態素解析し基本形への正規化が必要になる
- 日本語（中国語，韓国語）は，英語のようにスペースで区切られておらず，単語の単位がはっきりしない．
- 形態素解析器（単語分割）の活用

## 頻度計算

- 単純に形態素(トークン)ごとに分かち書きして集計, 語彙(タイプ)の出現回数を取得
  - 形態素は基本形に直すべき

### コーパス

- 英語
  - ブラウンコーパス
    - 5.8億トークン(単語)
    - 29万の語形タイプ

- グーグル
  - 1.0兆のトークン
  - 1358万の語形
  - スペルミス, 数字, 名前, 頭字語(略語)などを含んでいるため多く見える

## テキストの統計

- 単語の出現頻度の分布は統計的性質に従う
- 検索モデルやランキングアルゴリズムは統計的性質を考慮した設計が必要
  - ex. ) 重要な単語は，テキスト中の出現頻度が多いが，テキストの集合全体においては出現頻度は高くない

### Zipfの法則

- 単語の出現頻度の分布は，一様でなく語彙による偏りが存在している
  - `the`, `of`は、全単語の出現数の1割を占める
  - 一部の単語は突出して出現頻度が高いが多くの単語はめったに出現しない
- Zipfの法則とは:
  - 定数k ≒ ある単語の出現頻度f * 順位
    - 出現頻度が k 番目に大きい要素が、1位のものの頻度と比較して `1/k` に比例するという経験則(ジップ分布)
    - ここでの順位とは、単語の出現頻度の多い順
  - n回出現する単語の順位`r(n)`は`k/n`と推定できる
  - 出現頻度がn回の単語数`r(n)-r(n+1)`は`k/n(n+1)`
  - 出現頻度がn回の単語の出現割合は1/(k=n(n+1))
    - ex) 単語数500000で:
      - 出現1回の単語数は`k=1*2=2`として500000/2=250000(TRECで204357)
      - 出現2回の単語数は`k=2*3=6`として83333(67082)
      - 出現3回の単語数は`k=3*4=12`として41667(35083)

### ヒープの法則

- コーパスサイズと語彙数の関係
  - 語彙数v=定数k･単語数N^定数β
  - k, βはコーパスに依存
  - 単語数1k以下のコーパスでは成り立たながち

### 検索結果の文書数の推定

- ヒット数を見積もりたい
- クエリ"a b c"に対するヒット数f(a,b,c)=(Πf)/N^2
  - 単純にa, b, cが独立で存在しているページを含んでしまう
  - 共起確率がわかれば精度を高められる
- 途中まで順位付けした結果数からの推定
  - `全てのクエリを含む文書C/順位付けされた文書の全体に占める割合s`
- ２つの単語が独立と仮定しての推定
  - f(word1) * f(word2) / f(word1∧word2)
  - 実際は当然、推定より少ない

## 検索語の正規化

### トークン

- 検索語をトークンに分割して検索
  - 英語
    - 基本分かち書きでかんたん
    - `mal-formed`などのハイフンでの結合は？
    - URL、プログラム、スペルミスは無視したい
    - 大文字は小文字と異なる意味を持つ(固有名詞と一般名詞)

### ストップワードの除去

- ストップワード
  - それ自体意味を持たない機能語(冠詞，前置詞など)
  - 高出現頻度がち(the, a, of)
  - 検索には不要だしデータ容量も削減したいので除く
  - 例外) `to be or not to be`

- 方法は標準的なストップワードリストを参照する
  - しかしアプリケーションやドメイン、文書の種類によって差異がある
  - すべての単語を索引語とし、どの単語を利用するかをクエリの処理の際に判断したい

### ステミング

- 単語の形態変化/活用を共通の語幹(基本形)に変換し正規化したい
  - 索引語付けまたは，クエリ処理のタイミングで行う
  - 言語によって効果に差がある(英語: 5~10%UP, アラブ系: 50%)

- 手法は:
  - 関連語辞書ベース
  - アルゴリズムベース
    - xxxs->xxx, yyyed->yyy
    - 単純な方法だと検出漏れや誤検出に
      - [Porter Stemer](tartarus.org/~martin/PorterStemmer)の語幹生成
  - 辞書+アルゴ
    - Krovetz Stemer(単語生成, 辞書になければ，アルゴリズムで接尾辞を除去)
    - 検出誤りは少ないが，検出漏れは多い．

## フレーズ検索

- クエリが慣用句や決まり言葉などのフレーズの場合を考えたい
  - バラバラに検索するより1つの塊で検索したほうが良い
  - クエリがフレーズかどうかを判定したい
    - 品詞タグ付け or N-gram or 近接性オペレータを

### 品詞タグ付け

- テキストの統計を利用し，単語に品詞をタグ付け
- フレーズは名詞のグループとして定義

### 単語 N-グラム

- 品詞タグ付けは大規模な文書集合には時間がかかりすぎる
- フレーズはn語の単語列（N-グラム）
- N-グラム（ある長さまで）を索引とすると高速な検索ができる
  - しかし文書のサイズは肥大する(1000語1文書は2~5gramで約4倍に)
  - データセット: [Google N-gram](https://books.google.com/ngrams)

### 文書の構造とマークアップ言語

- HTMLタグによる文書構造の解析
  - ヘッダ，アンカーテキスト，強調されたテキスト
  - メタデータ(head.meta)も重要．
  - リンク
    - aタグ中のhrefとテキストノード(アンカーテキスト)

### アンカーテキスト

- リンク先のページの内容を記述していがち
- あるサイトについてのアンカーテキストの集合はめっちゃ良い
　　- ページランクより、アンカーテキストの方が効果大(クエリによる)

### ページランク

- リンク解析アルゴリズムでウェブページの重要度を測る
- 被リンクで人気を測る(論文の被引用=IFと同じ)
- リンクスパムのデータが弱点(無意味なサイトへのリンク)
- Web上の各ページをグラフノードとし、その重要度を確率として表す
  - あるWebページ集合でのページランクの和は1
  - ページランクの値はそのページへアクセスする確率を表す

### ランダムサーファーモデル

- ページランクが採用しているWeb探索アルゴリズム
  - 定数0<λ<1, r=rand[0,1]
  - r<λ ? ランダムなページに飛ぶ : 現在ページのリンク先からランダムに進む
  - これを繰り返す

### ページランクの計算

- Webページに固有の得点(ページランク)をもたせる
- 計算したいページuが被リンクを持っている各サイトについて
  - PR(v)/ページvからの出力エッジ(リンク)数L(v)を計算
  - PR(u)=PR(v)の総和
- これを、全ページの得点の和が1に収束するまでやる

### リンクの品質

- スパムによって影響を受ける
  - リンクファーム
    - ページランクを意図的に上げる目的で相互にリンクをはりまくるサイト群
  - トラックバック
    - リンクが張られたことを被リンクページ上で通知する機能
    - 元サイトとリンクを貼ったページとでループが生じる
  - コメント欄スパムでのリンク
    - no-followでページランクのクローラが辿れないようにする対策
