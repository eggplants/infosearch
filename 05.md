# 05: 意味が類似した検索語のとらえ方

## 同義（類義）関係，上位・下位関係

- 同義（類似）関係
  - ほとんどの（すべての）文脈で置き換え可能な単語
  - 車=自動車, CO2=二酸化炭素
- 上位下位関係
  - A ∋ B なら A は B の上位語
  - 哺乳類 ∋ {ネコ, イヌ, ヒト}
- 全体・部分関係
  - 顔={目, 鼻, 口}

## WordNet

- プリンストン大学で開発
- 意味関係に焦点を当てた辞書
- 155,287 語，117,659 の同義語集合

## 日本語ワードネット

- 情報通信研究機構が開発
- 9 万語

## 単語の分布仮説

- Zellig Harris さん (1954)

  - 「`oculist` と`eye-doctor` は，ほぼ同じ周囲語の中に出現」
  - 「単語 A と B がほぼ同じ周囲語の中に出現するのであれば，同義語と見なす」

- Firth さん (1957)
  - "You shall know a word by the company it keeps!"
  - 「単語の意味はその周囲の言葉により理解できるよ 🎶」

## 周囲語の分布による単語の意味の推定

- 単語の意味は周りの単語を見れば(だいたい)推測できる
  - 昨日もポニョを食べた．(ポニョ=食品？)
  - 朝見たらポニョが熟していた．(ポニョ=果実？)
  - ポニョにはレモンをちょっとかけるとうまい．(ポニョ=レモンが合う料理？)

## 単語間類似度と単語

### 文脈行列

- 周囲の単語(=文脈)の分布が似ている 2 単語は意味も似通っているはず！
  - 全単語について見るのはきつい

### 単語間共起行列 (word co-occurrence matrix)

- 見る「周囲」を大きさ(=窓幅, window size)と段落単位に絞った文脈行列
  - 窓幅が短ければ構文, 長ければ意味を重視
- サイズは語彙数|V|×|V|の疎行列となる
  - すっかすかなのでそこは無視して処理できるアルゴリズムがたくさんある

### 出現頻度による問題

- 単語間の関係を推定する上で出現頻度は十分に洗練された方法ではない
  - 頻度分布の偏りを考慮しなければならぬ
  - 高頻出の`of`, `the`などは意味があんまないので無視したい
- 正の相互情報量(P-PMI)
  - 文脈語が対象となる単語に対してより情報量を提供するか測る尺度
  - `2単語間PMI(w1, w2) = log₂{P(w1,w2)/P(w1)･P(w2)}`
  - P-PMI = (PMI.negative? ? 0 : PMI)

### 次元削減

- ベクトルの次元数を削減したい
  - データの広がりを反映した次元へ写像
  - データの本質的な差異を反映した重要な情報を残すことで圧縮
  - ほとんどの要素が 0 である行列（疎）を密なベクトルへ変換
  - 特異値分解（Single Value Decomposition, SVD）による次元削減
    - 直交行列 U に分解して，重要度の高い次元を残す

## 意味解析の問題点

- WordNet の利用の問題点
  - ニュアンスの欠落
    - 例：`good` の同義語として`proficient` が登録
  - ある文脈においてのみ同義  新語の更新をし続けることが困難
  - シソーラス作成者の主観が避けられない
  - シソーラスの構築や保守に人手が必要
- SVD の利用の問題点
  - n×m の行列に対して二乗のコストがかかる：O(m･n^2)
    - 何百万語や何百万文書を対象とすることが難しい
  - 新語や新しい文書を追加することが難しい

## 単語のベクトル表現

- ワンホットベクトル

  - 各単語を離散的に表現, 疎ベクトル
  - ベクトルの次元数=語彙数(巨大になる)
  - ワンホットベクトルでは類似度は測れない

- 単語ベクトル自身について，単語間の類似性をコード化するように学習したい
  - 単語の埋め込みを用いる

## 単語の埋め込み（Word Embedding）

- 類似した文脈に出現する単語ベクトルが類似するように単語の密なベクトルを構築すること

  - 圧倒的に圧縮できる

- Firth さん (1957)

  - 「単語の意味はその周囲の単語により理解できる」

- ある単語 w の複数の文脈（context）を使用して，単語 w の表現を構築
  - 文脈：ある単語の周囲（固定長の窓幅= 単語数）に出現する単語の集合

## Word2Vec

- 単語ベクトルを学習する枠組み
  - 対象となる単語`wt`が与えられた際に文脈となる単語`wt+j`が出現する確率 P を最大化するように，単語ベクトルを調整

### 利用

```python
# 4.x.y cannot work with models for Kyubyong/wordvectors
import json
from typing import List, Tuple, TypedDict

from gensim.models import Word2Vec  # gensim==3.8.3

# https://drive.google.com/open?id=0B0ZXk88koS2KVVNDS0lqdGNOSGM


class SimDict(TypedDict):
    term: str
    similarity: float


Sims = List[SimDict]


def get_synonyms(model: Word2Vec, input_word: str) -> Sims:
    """類似語の取得"""
    results = [{'term': word, 'similarity': sim}
               for word, sim in model.most_similar(input_word, topn=10)]
    return results


def calc_similarity(model: Word2Vec, t1: str, t2: str) -> float:
    """類似度計算"""
    return model.similarity(t1, t2)


def analogy(model: Word2Vec,
            relpair_x_y: Tuple[str, str], input_x: str) -> Sims:
    """アナロジー計算"""
    x, y = relpair_x_y
    results = [{"term": word, "similarity": sim}
               for word, sim in model.most_similar(positive=[y, input_x],
                                                   negative=[x], topn=10)]
    return results


def main() -> None:
    def p(d): return print(json.dumps(d, indent=4, ensure_ascii=False))

    model = Word2Vec.load('./data/ja.bin')

    syms = get_synonyms(model, 'アメリカ')
    res = analogy(model, ('フランス', 'パリ'), '日本')
    p(syms)
    p(res)


if __name__ == '__main__':
    main()
```
