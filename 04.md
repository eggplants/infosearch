# 04: 索引付け

## ページランク

- 重要度順にページをランク付けしたい
  - 被リンク数とリンク元の出リンク数に応じて決定

### 実装

-  DGL(Deep Graph Library)
  - 深層学習を用いたグラフNNモデル構築を行うパッケージ
  - ライセンスはApache License 2.0
  - NYU, 2018~

```python
import dgl
import networkx as nx
import numpy as np
import torch
from dgl import function as fn
from matplotlib import pyplot as plt


def build_simple_graph(show_plt: bool = False) -> dgl.DGLGraph:
    g = dgl.DGLGraph()
    g.add_nodes(3)
    src = np.array([0, 0, 1, 2])
    dst = np.array([1, 2, 2, 0])
    g.add_edges(src, dst)
    print("node:", g.number_of_nodes())
    print("edge:", g.number_of_edges())
    nx.draw(g.to_networkx(), node_size=3)
    if show_plt:
        plt.show()
    return g


def pagerank_builtin(g: dgl.DGLGraph, damp: float = 0.85) -> None:
    g.ndata['pv'] /= g.ndata['deg']
    g.update_all(message_func=fn.copy_src(src='pv', out='m'),
                 reduce_func=fn.sum(msg='m', out='m_sum'))
    N = g.number_of_nodes()
    g.ndata['pv'] = (1 - damp) / N + damp * g.ndata['m_sum']


def main() -> None:
    F = build_simple_graph()
    node_cnt = F.number_of_nodes()
    F.ndata['pv'] = torch.ones(node_cnt) / node_cnt
    F.ndata['deg'] = F.out_degrees(F.nodes()).float()

    print("init vals:", F.ndata['pv'])
    for _ in range(3):
        pagerank_builtin(F)
        print('step %d:' % _, F.ndata['pv'])


if __name__ == '__main__':
    main()
```

```bash
$ python pagerank.py
```

```python
Using backend: pytorch
node: 3
edge: 4
init vals: tensor([0.3333, 0.3333, 0.3333]) # 初期値1/3
step 0: tensor([0.3333, 0.1917, 0.4750])
step 1: tensor([0.4538, 0.1917, 0.3546])
step 2: tensor([0.3514, 0.2428, 0.4058])
```

## 索引

- サーチエンジンでページデータを高速に検索するためのDB上の索引(index)
  - 転置索引

### 索引の転置

- `索引語: 索引語が含まれる文書番号リスト(unique, order by 文書番号 or 単語スコア) or 出現頻度 or 文書中位置情報`の辞書
  - `Dict[str, List[int]]`
- 出現頻度: ランクアルゴリズム
- 位置情報: 照合
  - マッチした周辺の文章を取り出したいときに便利
  - ある2単語が隣接あるいは周辺にあるか確認するときに便利

### フィールドと索引付け

- 文書構造も検索には有効
  - フィールドの制約
    - 日付の情報を検索するには日付部分をみればよい
    - 見出し, 小見出し部分は重要度が高いので優先してみるなど
- フィールドを取り扱うための索引付け
  - フィールド種別索引リスト
  - 範囲リスト
    - 文書内の連続した領域
    - 	ex.) `文書番号:(開始位置,終了位置)`


### N-gram

```ruby
text="おはよう"
p text.chars.each_cons(2).map(&:join) #=>["おは", "はよ", "よう"]
```

### 部分文字列検索

- Suffix-arrayでの対数時間(log n)での検索

```ruby
text="さくらさくらいま"
(0..._=text.size).map{text[_1,_]}.sort
#=> ["いま", "くらいま", "くらさくらいま", "さくらいま",
#    "さくらさくらいま", "ま", "らいま", "らさくらいま"]
```

## 索引の圧縮

### なんで圧縮？

- 索引リストは非常に巨大がち
  - IndriサーチエンジンでTRECの文書集合を索引付け
    - 元の文書の25-50% の領域をとる
    - N-gramはさらに大きくなる

- 索引の圧縮によりディスクとメモリ空間を節約したい
  - リストを展開する必要があることに注意
  - 高い圧縮率と簡単な展開を実現する圧縮技術が必要

- lossless圧縮: 極端な高圧縮はできない

### 基本的な考え方

- 統計的に圧縮することで効率よい圧縮を実現
  - 共通するデータは短いコードに圧縮
  - 共通しないデータは長いコードにする

### 圧縮手法

- バイナリコードへの変換による符号化は解読不能/曖昧
- 差分符号化
  - 文書識別番号間の差(`d-gaps`) をコード化

```ruby
cal_d_gap=->inv{inv.each_cons(2).map{_1.inject(:-).abs}}
p cal_d_gap[[1, 5, 9, 18, 23, 24, 30, 44, 45, 48]]
#=> [4, 4, 9, 5, 1, 6, 14, 1, 3]
```

- 差分符号化の特徴
  - 高出現頻度単語は圧縮しやすい(d-gapsが小さい値がち)
  - 低出現頻度単語は圧縮しにくい(d-gapsが大きい値がち)

### スキップ操作

- 圧縮した転置リストからの検索では単純走査は非効率
  - スキップ操作で文書番号をチェックしたい
- スキップポインタ
  - スキップ操作を支援するデータ構造
  - `(文書番号d, 連結リスト(この場合は転置リスト)の位置p)`
  - スキップサイズをいい感じに設定するとよい

```ruby
# sorted
inv_lst = [1, 5, 9, 18, 23, 24, 30, 44, 45, 48]
mk_v_dgaps=->inv{
             inv.each_cons(2)
             .map{_1.inject(:-).abs}}
mk_p_skips=->inv,n{
             [*inv.each_with_index]
             .each_slice(n)
             .map(&:first)[1..]}
search_d=->d,gaps,skips{
             _m=skips[0][1]
             _d,_p=skips.filter{|a,_|a<=d}[-1]
             (_p..._p+_m).map{|i|
               _d==d ? (return i;break) : (_d+=gaps[i])}}
search_d[45,
         mk_v_dgaps[inv_lst],
         mk_p_skips[inv_lst,3]]
#=> 8
```

## 索引のマージ

- 大きな索引はメモリ領域の制限のため一度に作成できない
  - メモリ最大まで書込毎にディスクに書いてしメモリ開放
  - 処理終わりにマージ
  - マージのために索引の断片をいい感じに排列しておく(0-9a-zA-Zぁ-ん...)
    - MapReduce

## 索引付けの分散化

- ばかでかいWebの索引のためには安いPCを大量に用意して分散処理を行う
  - Google MapReduce
    - Apache Hadoop(Java)

### MapReduce

- Mapper
  - 文書データから単語, 位置を生成
  - 文書のURL、タイトルなどで得たHash値でMapperマシンを割当
- Reducer
  - 複数のMapperによって作られたデータを受け取り処理
  - Mapperの出力のHashが同じあるいはアルファベット順に近いキーの値を同じReducerマシンに割当
  - 同一のキーに対するすべての値（文書番号と位置）の組み合わせをファイルに書出
